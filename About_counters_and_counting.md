## About counters and counting: QoS - Quality of Service ## 

So, you've just deployed your latest application into production. Champaigns are popping and everybodyâ€™s happy. Next day the Devops team are approaching you. They do not look happy. The application failed crashed times last night during the busy events. And just to make things a little bit more complicated the reason of failure is still not known. The only hint you have is a high resources load on the OS level.

Yesterday, before you left for home everything was looking fine, the application was just running smoothly. Because you do not have enough time and the next round of busy events is just around the corner you decide to increase the resource capacity. This is not very complicated. You just build some new virtual machines, install the application and set them up into the load balancer. Everything is fine, the new instances start processing requests. Next day you learn that the cluster managed to pass the night successfully. The fire is out but the smoke is still there. You know that next major events are going to bring even more traffic. Most probably the current capacity is not going to be enough and the application will fail again. To increase the pressure even more the application owner noticed the situation and demands a fix.

By now it's clear that you need to define the application's capacity and the capacity required by the applications calling your API. You immediately engage with the maintainers of the client applications and after long debates you manage to establish a service level agreement. Based on these values the total capacity of the initial cluster should have been enough however the application's capacity was breached from the first night. Something's wrong! Currently you have no clue on the shape of your traffic for the current functionality and as well you know that new APIs are scheduled for the near future. This most probably will generate even more incoming traffic and most probably this will kill again your cluster. The only solution you have so far is to scale horizontally but this is not a long term solution. You need to figure out what is causing the resources exhausting. In order to do this you need a monitoring capability that will report the number of transactions per second for each operation called by each client. Of course, this feature needs to be highly performant, flexible and extensible.

Nothing new, right? What are the options?

You might get into the log files and parse the information out of these but you do not want to increase disk access as this is already a slow resources and even more based on such solutions the application will not be able to react easily. The data base might be another option but you do not want to increase the load on already busy component of your system. Another possibility might be to start a simple HTTP server embedded into your application solely meant to serve metrics about your application, but isn't this too much for your new requirement?

You need something simple!

JMX sounds like a good candidate. You'll need a simple singleton instance which will encapsulate a simple counter which will be incremented on each request fulfillment. This new instance will be published to JMX as a simple MBean. Ok, but this will only count the number of requests that the application is handling. This is not exactly what you were looking for as you would like to understand the number of requests performed for each operation. Because of this you need to instantiate this new abstraction for each operation. Things do get complicated because increasing counters all over the place in the business code will not scale and adding new features will only get matters more complicated. You need to take your abstraction even further. The API should allow simple counter increase based on operation identifier. Such an abstraction will allow isolating the counters from the business implementation. Sounds good! You implement this, deploy into production and call the API while monitoring the values. 

Now you are able to check how many times an operation was called. But wait, the initial intent was to have an understanding on how many requests per second are handled for each client. So the next step is to add another parameter to your abstraction identifying the caller's identity. Because you wisely chose to abstract out the collection of counters this should be fairly simple. The implementation will simply need to locate a counter and increment it. Of course, you do not want to have an implementation that knows at compile time all the clients because such solution is fragile and is not meant to last.

The implementation is required to be flexible enough so that it will accommodate at runtime any operation called by any client without having knowledge of the clients or operations sets. Sounds a bit complicated because such approach would imply sharing state - the collection of counters; between the multiple threads of the application. This structure needs to have some degree of mutability allowing the addition of counters as new requests for new clients are serviced. As base structure you could use a Map whose key is the identity of the caller concatenated with the operation that is being invoked while as value the counter would be enough for now. To address the issue of having a mutable collection shared by multiple threads rather than using a concurrent collection you choose to have a non-concurrent collection that is not mutated as shared but rather is recreated with the new counter.

Such an approach has disadvantages of course. It's possible that some counters will get created and lost by overriding by concurrent threads which will cause some requests to pass through as not counted. On the other side you get as well advantages. Over time the counter collection will stabilize, new entries will not be added and after this moment all threads will access a practically final not concurrent structure which has no performance penalties.

Now that you've figured out how to manage the mutability you need to find a way to have the counters measure requests per second rather than total number of calls. This means that each single counter needs to be reset every second. This brings a new problem: resetting the counter each second you need to associate to the counter a timestamp which will represent the time of the last reset which means more share state. Even more, besides adding the additional share state you will need to add a check which would verify the timestamp and reset before increment. These operation would be performed by the thread executing the client's request which will only add to the response time. This does not sound that nice! To fix this you might allow the request execution thread to simply increment the counter and for the sole purpose of resetting the counters you could have a new maintenance thread that would handle the reset concern. This means that the value of the counters collection needs to be a complex object that would encapsulate a circular list of counters which would be reused, each representing the number of calls for a single second. The maintenance thread will simply move the head of the circular list while the execution threads will just increment the counter in the head of the collection. Such approach will minimize contention between the maintenance and execution threads and will allow usage of practically immutable collections. Exposing the head of the list to JMX will allow monitoring of the number of requests per second for each client.

After deploying the implementation into production you immediately see why the application failed before. One of the clients is constantly breaching its quota. You raise this with the maintainers of the client application and after a few investigations it turns out that this was caused by a defect. Once the fix gets deployed into production the client's requests per second metric falls down. Perfect. Problem solved!

Is it really over? What if this happens again? I cannot afford to have another failure because one of the clients is misbehaving. You need to make sure that when a client is breaching its contract the other clients are not affected. This can be done by refusing to process the requests. Based on the current implementation all you need to add is the rejection path by checking the counter's value against the contract. When this happens a simple explanatory exception could be returned. Sounds a bit rough! Although you do not want to have clients that do not comply with the established contract and therefore you need to cut those above the limit it is possible that the application still has enough capacity to handle the requests that are above the limit. You would like to reject requests only when processing such calls would affect the processing of the other legitimate ones. To have this you could add an additional clause in the decision to reject a requests, such as verifying the total available capacity or checking the size of the requests queues. Such implementation requires further computation and adding it should happen only when it is justifiable in terms of cost.

Beside this enhancement you could go even further. You might want to grant execution over the established contract when the client only does e few transactions per second above the limit only occasionally. This means that you will need the compute the average for the past few seconds. In order to keep the computation of this out of the request execution thread the average could be computed by the maintenance thread, once every second. This is possible due to the fact that for each operation and client the abstraction contains the list of counter for the last seconds.

*Voila*! Once you handle these last bits you've got yourself a pretty solid QoS - Quality of Service implementation.

