## About counters and counting: QoS - Quality of Service  ##

So, you've just deployed your latest application into production. Champaigns are popping and everybodyâ€™s happy. Next day the Devops team are approaching you. They do not look happy. The application failed several times last night during the busy events and just to make things a little bit more complicated the reason of the failure is still not known. The only hint you have is a high resources load on the OS level.

Yesterday, before you left for home everything was looking fine, the application was running smoothly. Because you do not have enough time and the next round of events you decide to increase the physical resource capacity. This is not very complicated. You just build some new virtual machines, install the application and set them up into the load balancer. Everything is fine, the new instances start processing requests. Next day you learn that the cluster managed to pass the night successfully. The fire is out but the smoke is still there. You know that next major events are going to bring even more traffic. Most probably the current capacity is not going to be enough and the application will fail again. To increase the pressure even more, the application owner noticed the situation and demands a fix.

By now it's clear that you need to define the application's required processing capacity. You immediately engage with the maintainers of the client applications that are using your API and after long debates you manage to establish a service level agreement. Based on these values the total capacity of the initial cluster should have been enough, however the application was overwhelmed from the first night. Something is wrong! Currently you have no clue on the shape of your incoming traffic for the current functionality and as well, you know that new functionality is scheduled for the near future. This most probably will generate even more incoming traffic and most probably this will kill your cluster again. The only solution you have so far is to scale horizontally but this is not a long term fix. You need to figure out what is causing the resources exhausting. In order to do this you need a monitoring capability that will report the number of transactions per second for each operation called by each client. Of course, this feature needs to be highly performant, flexible and extensible.

Nothing new, right? What are the options?

You might get into the log files and parse the information out of these but you do not want to increase disk access as this is already a slow resources and even more, based on such solution the application will not be able to react instantly. The data base might be another option but you do not want to increase the load on already busy component of your system. Another possibility might be to start a simple HTTP server embedded into your application solely meant to serve metrics about your application, but isn't this too much for your new requirement?

You need something simple!

JMX sounds like a good candidate. You'll need a simple singleton instance which will encapsulate a simple counter that will be incremented on each request fulfillment. This new instance will be published to JMX as a simple MBean. Ok, but this will only count the number of requests that the application is handling which is not exactly what you were looking for. You would like to measure the number of requests performed for each operation by each client per second. This can be achieved by instantiating the new abstraction once for each operation. Things do get complicated because increasing counters all over the place in the business code will not scale and adding new features will only get matters more complicated. You need to take your abstraction even further. The API should allow simple counter increase based on operation identifier, leaving the counter routing into the responsibility of your implementation. Such an abstraction will allow isolating the counters from the business implementation. Sounds good! You implement this, deploy into production and call the API while monitoring the values. 

Now you are able to check how many times an operation was called. But wait, the initial intent was to have an understanding on how many requests per second are handled for each client. So the next step is to add another parameter to your abstraction identifying the caller's identity and because you wisely chose to abstract out the collection of counters this should be fairly simple. The implementation will just determine the counter associated to the given operation and client, and increment it. Of course, you do not want to have an implementation that knows at compile time all the clients. Such solution would be fragile and would not last in time.

The implementation is required to be flexible enough so that it will accommodate at runtime any operation called by any client without having knowledge of the clients or operations sets. Sounds a bit complicated because such approach would imply sharing state - the collection of counters; between the multiple threads of the application. This structure needs to have some degree of mutability allowing the addition of new counters as requests triggered by new clients are serviced. As base structure you could use a Map whose keys are composed by the identity of the caller concatenated with the operation name that is being invoked. As value the counter would be enough for now. To address the issue of having this collection mutable and shared by multiple threads rather than using a concurrent collection you choose to have a non-concurrent collection that is not mutated as shared but rather is recreated with new counters each time a new counter is required.

Such an approach has some disadvantages. It's possible that some counters will get created and lost by overriding by concurrent threads. This will cause some requests to pass through as counted by counters that would be thrown away. On the other side you get as well advantages. Over time the counter collection will stabilize as less and less thread would discover new counters and therefore would not require overriding the collection of counter. After the stabilization all threads will access a practically final not concurrent structure which has no performance penalties.

Now that you've figured out how to manage the mutability you need to find a way to have the counters measure requests per second rather than total number of calls. This means that each single counter needs to be reset every second. This brings a new problem: resetting the counter each second will imposes that each counter is associated with a timestamp that represents the time of the last reset. This means more share state. Even more, besides adding the additional timestamp you will need to add the check for the timestamp verifying if reset is required. These operation would be performed by the thread executing the client's request and this will only add to the response time. This does not sound very nice! To fix it you might allow the request execution thread to simply increment the counter and for the sole purpose of resetting the counters you could have a new maintenance thread that would handle the reset concern. So that you do not loose counted requests, the value of the counters collection needs to be a complex object that would encapsulate a circular list of counters each representing the number of calls recorded for a single second. The maintenance thread will simply move the head of the circular list while the execution threads will just increment the counter in the head of the collection. Such approach will minimize contention between the maintenance and execution threads and will allow usage of practically immutable collections. Exposing the head of the list to JMX will allow monitoring of the number of requests per second for each client.

After deploying the implementation into production you immediately see why the application failed before. One of the clients is constantly breaching its quota. You raise this with the maintainers of the client application and after a few investigations it turns out that this was caused by a defect in the client application. Once the fix gets deployed into production the requests per second metric falls down bellow the expected range. Perfect. Problem solved!

Is it really over? What if this happens again? You cannot afford to have another failure because one of the clients is misbehaving. You need to make sure that when a client is breaching its contract the other clients are not affected. This can be done by refusing to process the requests that are over the agreed quota. Based on the current implementation all you need to add is the rejection path. This can be done by checking the counter's value against the contract before doing the actual incrementation. When the client passes the limit a simple explanatory exception should be returned. Sounds a bit rough! Although you do not want to have clients that do not comply with the established contract and reject their requests, it is possible that the application still has enough capacity to handle those requests that are above the limit. You would like to reject requests only when processing such calls would affect the processing of the other legitimate ones. To have this you could add an additional clause in the decision to reject a requests, such as verifying the total available capacity or checking the size of the request queues. Such implementation requires further computation and adding it should happen only when it is justifiable in terms of cost.

Beside this enhancement you could go even further. You might want to grant execution over the established contract when the client only does a few transactions per second above the limit only occasionally. This means that you will need the compute the average for the past few seconds. This implies that an average for the past seconds needs to be computed. In order to keep the computation of this out of the request execution thread the average could be computed by the maintenance thread, once every second. Moreover, because you choose to have the counters reused, you already have a small historical evolution which will allow you to compute the average for the last seconds.

*Voila*! Once you handle these last bits you've got yourself a pretty solid QoS - Quality of Service implementation.

